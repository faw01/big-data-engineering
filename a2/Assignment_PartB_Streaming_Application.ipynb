{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "import pprint\n",
    "from kafka3 import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory (local machine only)\n",
    "# os.chdir(\"A2\")\n",
    "# os.chdir(\"data\")\n",
    "\n",
    "# dirs = os.listdir(os.getcwd())\n",
    "# print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostip = \"192.168.10.125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode latitude and longitude into a geohash with specified precision\n",
    "def geohash_encode(latitude, longitude, precision=3):\n",
    "\n",
    "    return geohash2.encode(latitude, longitude, precision)\n",
    "\n",
    "\n",
    "# check if two geohashes are within proximity based on specified precision\n",
    "\n",
    "\n",
    "def is_within_proximity(geohash1, geohash2, precision):\n",
    "    # compare the first 'precision' characters of the geohashes\n",
    "\n",
    "    return geohash1[:precision] == geohash2[:precision]\n",
    "\n",
    "\n",
    "# determine the cause of the fire based on air temperature and GHI\n",
    "\n",
    "\n",
    "def determine_fire_cause(air_temperature, ghi):\n",
    "    # if air temperature > 20 and GHI > 180, fire cause is \"natural\", otherwise \"other\"\n",
    "\n",
    "    return \"natural\" if air_temperature > 20 and ghi > 180 else \"other\"\n",
    "\n",
    "\n",
    "def sync_fix(hotspots):\n",
    "    # merge hotspots that are within proximity and have similar timestamps\n",
    "\n",
    "    merged_hotspots = []\n",
    "    # iterate through length of hotspots\n",
    "\n",
    "    for i in range(len(hotspots)):\n",
    "        # if hotspot is None, skip\n",
    "        if hotspots[i] is None:\n",
    "\n",
    "            continue\n",
    "        # create a copy of the hotspot\n",
    "\n",
    "        merged_hotspot = hotspots[i].copy()\n",
    "\n",
    "        count = 1\n",
    "        # create a list of hotspots that are being merged\n",
    "\n",
    "        merging_hotspots = [hotspots[i]]\n",
    "        # iterate through the rest of the hotspots\n",
    "\n",
    "        for j in range(i + 1, len(hotspots)):\n",
    "            # if hotspot is None, skip\n",
    "\n",
    "            if hotspots[j] is None:\n",
    "\n",
    "                continue\n",
    "            # check if hotspots[i] and hotspots[j] are within proximity (geohash precision 4) and their time difference is within 10 minutes\n",
    "\n",
    "            if is_within_proximity(\n",
    "                hotspots[i][\"geohash5\"], hotspots[j][\"geohash5\"], 4\n",
    "            ) and abs(\n",
    "                datetime.strptime(hotspots[i][\"datetime\"], \"%H:%M:%S\")\n",
    "                - datetime.strptime(hotspots[j][\"datetime\"], \"%H:%M:%S\")\n",
    "            ) <= timedelta(\n",
    "                minutes=10\n",
    "            ):\n",
    "                # if conditions are met, merge the hotspots, add confidence and surface temperature of hotspots[j] to merged_hotspot\n",
    "\n",
    "                merged_hotspot[\"confidence\"] += hotspots[j][\"confidence\"]\n",
    "\n",
    "                merged_hotspot[\"surface_temperature_celcius\"] += hotspots[j][\n",
    "                    \"surface_temperature_celcius\"\n",
    "                ]\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                merging_hotspots.append(hotspots[j])\n",
    "\n",
    "                hotspots[j] = None\n",
    "\n",
    "        if count > 1:\n",
    "            # if multiple hotspots were merged, calculate the average confidence and surface temperature\n",
    "\n",
    "            merged_hotspot[\"confidence\"] = round(merged_hotspot[\"confidence\"] / count)\n",
    "\n",
    "            merged_hotspot[\"surface_temperature_celcius\"] = round(\n",
    "                merged_hotspot[\"surface_temperature_celcius\"] / count\n",
    "            )\n",
    "\n",
    "            merged_hotspots.append(merged_hotspot)\n",
    "\n",
    "            print(\"========== Sync Fix: Merging Hotspots ==========\")\n",
    "\n",
    "            print(f\"Merging {count} hotspots:\")\n",
    "\n",
    "            pprint.pprint(merging_hotspots)\n",
    "\n",
    "            print(\"Merged Hotspot:\")\n",
    "\n",
    "            pprint.pprint(merged_hotspot)\n",
    "\n",
    "            print(\"================================================\")\n",
    "\n",
    "        else:\n",
    "            # if no merging occurred, add the original hotspot to merged_hotspots\n",
    "\n",
    "            merged_hotspots.append(hotspots[i])\n",
    "    return merged_hotspots\n",
    "\n",
    "\n",
    "def process_climate_data(data):\n",
    "    # proces the climate data\n",
    "\n",
    "    climate_data = {\n",
    "        \"station\": 948700,\n",
    "        \"date\": datetime.strptime(data[\"created_date\"], \"%Y-%m-%d\"),\n",
    "        \"climate\": {\n",
    "            \"air_temperature_celcius\": data[\"air_temperature_celcius\"],\n",
    "            \"relative_humidity\": data[\"relative_humidity\"],\n",
    "            \"windspeed_knots\": data[\"windspeed_knots\"],\n",
    "            \"max_wind_speed\": data[\"max_wind_speed\"],\n",
    "            \"precipitation\": data[\"precipitation \"],\n",
    "            \"ghi_wm2\": data[\"GHI_w/m2\"],\n",
    "        },\n",
    "        \"geohash3\": geohash_encode(data[\"latitude\"], data[\"longitude\"], precision=3),\n",
    "        \"geohash5\": geohash_encode(data[\"latitude\"], data[\"longitude\"], precision=5),\n",
    "        \"hotspots\": [],\n",
    "    }\n",
    "\n",
    "    print(\"========== Climate Data Processing ==========\")\n",
    "\n",
    "    pprint.pprint(climate_data)\n",
    "\n",
    "    print(\"=============================================\")\n",
    "    return climate_data\n",
    "\n",
    "\n",
    "def process_hotspot_data(data):\n",
    "    # process the hotspot data\n",
    "\n",
    "    hotspot_data = {\n",
    "        \"latitude\": data[\"latitude\"],\n",
    "        \"longitude\": data[\"longitude\"],\n",
    "        \"confidence\": data[\"confidence\"],\n",
    "        \"surface_temperature_celcius\": data[\"surface_temperature_celcius\"],\n",
    "        \"datetime\": data[\"created_time\"],\n",
    "        \"geohash3\": geohash_encode(data[\"latitude\"], data[\"longitude\"], precision=3),\n",
    "        \"geohash5\": geohash_encode(data[\"latitude\"], data[\"longitude\"], precision=5),\n",
    "    }\n",
    "\n",
    "    print(\"========== Hotspot Data Processing ==========\")\n",
    "\n",
    "    pprint.pprint(hotspot_data)\n",
    "\n",
    "    print(\"=============================================\")\n",
    "    return hotspot_data\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    # process data for insertion into mongodb\n",
    "\n",
    "    document = {}\n",
    "\n",
    "    document[\"date\"] = data[\"date\"].isoformat()\n",
    "    document[\"station\"] = data[\"station\"]\n",
    "\n",
    "    document[\"climate\"] = {\n",
    "        \"air_temperature_celcius\": data[\"climate\"][\"air_temperature_celcius\"],\n",
    "        \"relative_humidity\": data[\"climate\"][\"relative_humidity\"],\n",
    "        \"windspeed_knots\": data[\"climate\"][\"windspeed_knots\"],\n",
    "        \"max_wind_speed\": data[\"climate\"][\"max_wind_speed\"],\n",
    "        \"precipitation\": data[\"climate\"][\"precipitation\"],\n",
    "        \"ghi_wm2\": data[\"climate\"][\"ghi_wm2\"],\n",
    "    }\n",
    "\n",
    "    if \"fire_cause\" in data:\n",
    "        document[\"climate\"][\"cause\"] = data[\"fire_cause\"]\n",
    "\n",
    "    if \"hotspots\" in data:\n",
    "        document[\"hotspots\"] = []\n",
    "\n",
    "        for each in data[\"hotspots\"]:\n",
    "\n",
    "            hotspot = {}\n",
    "            # convert datetime string to time object and format as ISO string\n",
    "\n",
    "            hotspot[\"datetime\"] = (\n",
    "                datetime.strptime(each[\"datetime\"], \"%H:%M:%S\").time().isoformat()\n",
    "            )\n",
    "            hotspot[\"confidence\"] = each[\"confidence\"]\n",
    "            hotspot[\"latitude\"] = each[\"latitude\"]\n",
    "            hotspot[\"longitude\"] = each[\"longitude\"]\n",
    "            hotspot[\"surface_temperature_celcius\"] = each[\"surface_temperature_celcius\"]\n",
    "\n",
    "            document[\"hotspots\"].append(hotspot)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "\n",
    "    print(f\"=============== Processing Batch: {batch_id} ===============\")\n",
    "\n",
    "    climate_data = None\n",
    "    hotspot_data = []\n",
    "\n",
    "    for row in batch_df.collect():\n",
    "\n",
    "        data = json.loads(row[\"value\"])\n",
    "\n",
    "        if data[\"producer_id\"] == \"climate_producer\":\n",
    "\n",
    "            if climate_data is None:\n",
    "                # if climate data is not set, process the current data\n",
    "\n",
    "                climate_data = process_climate_data(data)\n",
    "\n",
    "            else:\n",
    "                # if climate data is already set, keep the earliest one and drop the rest\n",
    "\n",
    "                print(\n",
    "                    \"Multiple climate reports received in the batch. Keeping the earliest one and dropping the rest.\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # if data is from hotspot producers, process the hotspot data\n",
    "\n",
    "            hotspot_data.append(process_hotspot_data(data))\n",
    "\n",
    "    if climate_data:\n",
    "        # filter hotspots based on proximity to climate data (geohash precision 3)\n",
    "        climate_data[\"hotspots\"] = [\n",
    "            hotspot\n",
    "            for hotspot in hotspot_data\n",
    "            if is_within_proximity(hotspot[\"geohash3\"], climate_data[\"geohash3\"], 3)\n",
    "        ]\n",
    "        # Merge hotspots that are within proximity and time difference\n",
    "\n",
    "        climate_data[\"hotspots\"] = sync_fix(climate_data[\"hotspots\"])\n",
    "\n",
    "        if climate_data[\"hotspots\"]:\n",
    "            # if there are hotspots, determine the fire cause based on air temperature and GHI\n",
    "\n",
    "            climate_data[\"fire_cause\"] = determine_fire_cause(\n",
    "                float(climate_data[\"climate\"][\"air_temperature_celcius\"]),\n",
    "                float(climate_data[\"climate\"][\"ghi_wm2\"]),\n",
    "            )\n",
    "\n",
    "        print(\"========== Final Climate Data ==========\")\n",
    "\n",
    "        pprint.pprint(climate_data)\n",
    "\n",
    "        print(\"========================================\")\n",
    "\n",
    "        # process for mongo\n",
    "\n",
    "        processed_data = process_data(climate_data)\n",
    "\n",
    "        print(\"========== !Processed Data! ==========\")\n",
    "\n",
    "        pprint.pprint(processed_data)\n",
    "\n",
    "        print(\"========================================\")\n",
    "\n",
    "        # insert processed data into MongoDB\n",
    "\n",
    "        client = MongoClient(hostip, 27017)\n",
    "\n",
    "        db = client[\"fit3182_assignment_db\"]\n",
    "\n",
    "        collection = db[\"processed_data\"]\n",
    "\n",
    "        collection.insert_one(processed_data)\n",
    "\n",
    "        print(\"Data inserted into MongoDB\")\n",
    "\n",
    "        client.close()\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"No climate data received in the batch.\")\n",
    "\n",
    "    print(f\"=============== Batch {batch_id} Processing Completed ===============\")\n",
    "\n",
    "\n",
    "# engine\n",
    "if __name__ == \"__main__\":\n",
    "    # setup spark session\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"StopFire\").getOrCreate()\n",
    "\n",
    "    # read data from kafka\n",
    "    kafka_sdf = (\n",
    "        spark.readStream.format(\"kafka\")\n",
    "        # subscribe to climate, hotspot_aqua, and hotspot_terra topics\n",
    "        .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "        .option(\"subscribe\", \"climate,hotspot_aqua,hotspot_terra\")\n",
    "        # start from the latest offset instead of earliest\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    # convert value column to string and parse as json\n",
    "    streaming_df = kafka_sdf.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    # start the query\n",
    "    query = (\n",
    "        streaming_df.writeStream.outputMode(\"append\")\n",
    "        # pass each batch to process_batch function\n",
    "        .foreachBatch(process_batch).start()\n",
    "    )\n",
    "\n",
    "    # wait for the query to terminate\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "# connect to mongodb\n",
    "client = MongoClient(hostip, 27017)\n",
    "\n",
    "# get appropriate database\n",
    "db = client[\"fit3182_assignment_db\"]\n",
    "\n",
    "# get proper collection\n",
    "collection = db[\"processed_data\"]\n",
    "\n",
    "# get all documents in the collection\n",
    "cursor = collection.find({})\n",
    "\n",
    "# print each document to see\n",
    "for document in cursor:\n",
    "    pprint(document)\n",
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
