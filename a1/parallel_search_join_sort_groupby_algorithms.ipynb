{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRWxd1YrHdSB"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "For this test, we will use the following tables R and S to write solutions to three parallel algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImhBOqgmHdSC"
   },
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "\n",
    "\n",
    "R = [\n",
    "    (\"Adele\", 8),\n",
    "    (\"Bob\", 22),\n",
    "    (\"Clement\", 16),\n",
    "    (\"Dave\", 23),\n",
    "    (\"Ed\", 11),\n",
    "    (\"Fung\", 25),\n",
    "    (\"Goel\", 3),\n",
    "    (\"Harry\", 17),\n",
    "    (\"Irene\", 14),\n",
    "    (\"Joanna\", 2),\n",
    "    (\"Kelly\", 6),\n",
    "    (\"Lim\", 20),\n",
    "    (\"Meng\", 1),\n",
    "    (\"Noor\", 5),\n",
    "    (\"Omar\", 19),\n",
    "]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "\n",
    "\n",
    "S = [\n",
    "    (\"Arts\", 8),\n",
    "    (\"Business\", 15),\n",
    "    (\"CompSc\", 2),\n",
    "    (\"Dance\", 12),\n",
    "    (\"Engineering\", 7),\n",
    "    (\"Finance\", 21),\n",
    "    (\"Geology\", 10),\n",
    "    (\"Health\", 11),\n",
    "    (\"IT\", 18),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W3uUvJLErBY",
    "tags": []
   },
   "source": [
    "## 1. Parallel Searching Algorithm (2 marks)\n",
    "\n",
    "**This section consist of 4 sub-questions.**\n",
    "\n",
    "In this task, you will build a **parallel search algorithm for range selection (continuous)** for a given query.\n",
    "\n",
    "**Implement a parallel search algorithm** that uses local linear search (i.e., `linear_search()`) and is able to work with the hash partitioning method (i.e., `h_partition()`).\n",
    "**Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvTYiOkyFJp8"
   },
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "\n",
    "\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list OR numpy array)\n",
    "    key -- an query attribute (number)\n",
    "\n",
    "    Return:\n",
    "    result --  list of matched records such as [('Adele', 8)]\n",
    "                or the position of searched record\n",
    "                whichever is more appropriate for your code\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # iterate over each record in input dataset\n",
    "\n",
    "    for record in data:\n",
    "\n",
    "        # check if number attribute of record matches key\n",
    "\n",
    "        if record[1] == key:\n",
    "\n",
    "            # match found - append the record to the matches list\n",
    "\n",
    "            matches.append(record)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1555295423504,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "ruGmDOnfmPWc",
    "outputId": "494f8520-91c4-470d-e640-0bc5a6e184f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lim', 20)]\n"
     ]
    }
   ],
   "source": [
    "results = linear_search(R, 20)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hash Partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An_0xFW2FQvs"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "\n",
    "\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record attribute (int)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    return x % n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMLZCK2BFYeF"
   },
   "outputs": [],
   "source": [
    "# Hash data partitioning function.\n",
    "# We will use the s_hash function defined above\n",
    "\n",
    "\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    n -- the number of processors (int)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    partitions = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # iterate the logic for each data record\n",
    "\n",
    "    for record in data:\n",
    "\n",
    "        # get the hash key of the input\n",
    "\n",
    "        hash_key = s_hash(record[1], n)\n",
    "\n",
    "        # if the key exists\n",
    "\n",
    "        if hash_key in partitions.keys():\n",
    "\n",
    "            # get the value list of the key\n",
    "\n",
    "            partition_records = partitions[hash_key]\n",
    "\n",
    "            # add the new input to the list\n",
    "\n",
    "            partition_records.append(record)\n",
    "\n",
    "            # update the key with the new value list\n",
    "\n",
    "            partitions[hash_key] = partition_records\n",
    "        # if the key does not exist\n",
    "\n",
    "        else:\n",
    "\n",
    "            # create an empty value list\n",
    "\n",
    "            partition_records = list()\n",
    "\n",
    "            # add the input to the list\n",
    "\n",
    "            partition_records.append(record)\n",
    "\n",
    "            # add the value list to the key\n",
    "\n",
    "            partitions[hash_key] = partition_records\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1555295448743,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "Ako86IBFpqwj",
    "outputId": "dd8f1dc8-a73f-4455-f059-58cc0e79a01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('Fung', 25), ('Lim', 20), ('Noor', 5)],\n",
      " 1: [('Clement', 16), ('Ed', 11), ('Kelly', 6), ('Meng', 1)],\n",
      " 2: [('Bob', 22), ('Harry', 17), ('Joanna', 2)],\n",
      " 3: [('Adele', 8), ('Dave', 23), ('Goel', 3)],\n",
      " 4: [('Irene', 14), ('Omar', 19)]}\n"
     ]
    }
   ],
   "source": [
    "partitions = h_partition(R, 5)\n",
    "pprint(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parallel Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlVKTCO-FkV9"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def collect_result(results, result):\n",
    "    # Callback for when linear_search returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "\n",
    "\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key.\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset (list)\n",
    "    query_range -- a tuple with inclusive range end-points (e.g. [30, 50] means the interval 30-50)\n",
    "    n_processor -- the number of parallel processors (int)\n",
    "\n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # create an empty list to store the results\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # partition the data using hash function\n",
    "\n",
    "    partitioned_data = h_partition(data, n_processor)\n",
    "\n",
    "    # iterate over the query range\n",
    "\n",
    "    for query in range(query_range[0], query_range[1] + 1):\n",
    "\n",
    "        # get the hash key of the query\n",
    "\n",
    "        query_hash = s_hash(query, n_processor)\n",
    "\n",
    "        # get the data records of the hash key and convert it to a list\n",
    "\n",
    "        range_data = list(partitioned_data[query_hash])\n",
    "\n",
    "        # apply linear search on the data records\n",
    "\n",
    "        result = pool.apply_async(linear_search, [range_data, query])\n",
    "\n",
    "        # add an if condition because its appending regardless, so there are eempty lists\n",
    "\n",
    "        if result.get():\n",
    "            # append the result to the results list\n",
    "\n",
    "            results.append(result.get())\n",
    "    # close the pool\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    # wait for the worker processes to terminate\n",
    "\n",
    "    pool.join()\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1555295758812,
     "user": {
      "displayName": "Prajwol Sangat",
      "photoUrl": "https://lh4.googleusercontent.com/-jjsf-xy1nFE/AAAAAAAAAAI/AAAAAAAAAmM/hDKToPDDstc/s64/photo.jpg",
      "userId": "16045204576665706371"
     },
     "user_tz": -600
    },
    "id": "vgfN1IcyFxaG",
    "outputId": "5fa6eb31-b14b-441e-fdad-a6767369df34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Noor', 5)], [('Kelly', 6)], [('Adele', 8)], [('Ed', 11)], [('Irene', 14)]]\n"
     ]
    }
   ],
   "source": [
    "n_processor = 3\n",
    "results = parallel_search_range(R, [5, 15], n_processor)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Parallel Search Variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly answer the following.**\n",
    "\n",
    "1. How would the parallel search you implemented need to be changed if we switched to round-robin partition for the data? Provide a small snippet of code with the modified search loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers**:\n",
    "\n",
    "To switch to using round-robin partitioning instead of hash partitioning, we would need to:\n",
    "\n",
    "1. Replace the function call `h_partition()` with `rr_partition()` as this is the round-robin partitioning function. The `rr_partition()` function would divide the data in each record in turn as it is allocated to a processing element in a clockwise manner. For example, it would divide the first record of a table to be partitioned is distributed to the first processing element, the second record to the second processing element, and so on.\n",
    "\n",
    "2. The modified search loop would iterate over each processor's sublist of data, submit a parallel linear search task for the query on each processor's data subset using `pool.apply_async()`, and collect the results from each processor, appending them to the final results list.\n",
    "\n",
    "```python\n",
    "partitioned_data = rr_partition(data, n_processor)\n",
    "\n",
    "for query in range(query_range[0], query_range[1] + 1):\n",
    "    results_per_processor = []\n",
    "    for processor_data in partitioned_data:\n",
    "        result = pool.apply_async(linear_search, [processor_data, query])\n",
    "        results_per_processor.append(result)\n",
    "\n",
    "    for result in results_per_processor:\n",
    "        if result.get():\n",
    "            results.append(result.get())\n",
    "```\n",
    "\n",
    "Using `rr_partition()` divides it among the processors and hence, for each query value, we iterate over each processor's sublist of data, submitting a parallel linear search task for the query on each processor's data subset using `pool.apply_async()` and store the result objects in `results_per_processor`. After submitting the tasks for all processors, we iterate over the result objects, retrieve the actual results using `result.get()`, and append the non-empty results to the final `results` list.\n",
    "\n",
    "Using round-robin partitioning ensures that the data is distributed evenly across the processors. However, the search operation needs to be performed on each processor's data subset for every query value. This may be less efficient compared to using hash partitioning, where the search can be limited to a specific processor based on the hash value of the query. Round-robin partitioning guarantees better load balancing among the processors, as each processor receives an equal amount of data to process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf29MfmUGWV1"
   },
   "source": [
    "## 2. Parallel Join Algorithm (4 marks)\n",
    "\n",
    "**This section consist of 5 sub-questions.**\n",
    "\n",
    "In this task, you will implement a **disjoint-partitioning based parallel join algorithm**. This algorithm consist of two stages.\n",
    "\n",
    "1. Data disjoint-partitioning across processors.\n",
    "1. Local join in each processor.\n",
    "\n",
    "As a data partitioning method, use the range partitioning algorithm (i.e. `range_partition( )`). Assume that we have **3 parallel processors**, processor 1 will get records with join attribute value between 1 and 9, processor 2 between 10 and 19, and processor 3 between 20 and 29. Note that both tables R and S need to be partitioned using the same ranges described earlier.\n",
    "\n",
    "As a joining technique, use the hash based join algorithm (i.e., `HB_join( )`). **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Range Partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_BwmzrmHaTN"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "\n",
    "\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list)\n",
    "    range_indices -- the range end-points (e.g., [5, 10, 15] means\n",
    "                        4 intervals: x < 5, 5 <= x < 10, 10 <= x < 15, 15 <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # copy the data to a new list\n",
    "\n",
    "    partitioned_data = list(data)\n",
    "\n",
    "    # sort the data based on the join attribute\n",
    "\n",
    "    partitioned_data.sort(key=lambda x: x[1])\n",
    "\n",
    "    # get the number of bins\n",
    "\n",
    "    n_bins = len(range_indices)\n",
    "\n",
    "    # iterate over the range indices\n",
    "\n",
    "    for i in range(n_bins):\n",
    "\n",
    "        # get the data records within the current range\n",
    "\n",
    "        s = [x for x in partitioned_data if x[1] < range_indices[i]]\n",
    "\n",
    "        # add the data records to the result list\n",
    "\n",
    "        result.append(s)\n",
    "\n",
    "        # get the last element of the current range\n",
    "\n",
    "        last_element = s[len(s) - 1]\n",
    "\n",
    "        # get the index of the last element\n",
    "\n",
    "        last_index = partitioned_data.index(last_element)\n",
    "\n",
    "        # update the data records\n",
    "\n",
    "        partitioned_data = partitioned_data[int(last_index) + 1 :]\n",
    "    # add the remaining data records to the result list\n",
    "\n",
    "    result.append([x for x in partitioned_data if x[1] >= range_indices[n_bins - 1]])\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4eA2G_7vafZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Meng', 1),\n",
      "  ('Joanna', 2),\n",
      "  ('Goel', 3),\n",
      "  ('Noor', 5),\n",
      "  ('Kelly', 6),\n",
      "  ('Adele', 8)],\n",
      " [('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19)],\n",
      " [('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(range_partition(R, [10, 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hash Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kONrahsIMmD"
   },
   "outputs": [],
   "source": [
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works\n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute.\n",
    "\n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the value of the join attribute into the digits\n",
    "\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "\n",
    "    # Calulate the sum of elemenets in the digits\n",
    "\n",
    "    return sum(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpFbToJIPlr"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute (in second position) in the input tables T1 & T2.\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    dic = {}  # We will use a dictionary\n",
    "\n",
    "    # For each record in table T2\n",
    "\n",
    "    for s in T2:\n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "\n",
    "        s_key = H(s)\n",
    "        if s_key in dic:\n",
    "            dic[s_key].add(s)  # If there is an entry\n",
    "        else:\n",
    "            dic[s_key] = {s}\n",
    "    print(dic)\n",
    "    # For each record in table T1 (probing)\n",
    "\n",
    "    for r in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "\n",
    "        r_key = H(r)\n",
    "        # If an index entry is found Then\n",
    "\n",
    "        if r_key in dic:\n",
    "            # Compare each record on this index entry with the record of table T1\n",
    "\n",
    "            for item in dic[r_key]:\n",
    "                if item[1] == r[1]:\n",
    "                    # Put the rsult\n",
    "\n",
    "                    result.append((r[0], r[1], item[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXx6eixAvwPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: {('Arts', 8)}, 6: {('Business', 15)}, 2: {('Health', 11), ('CompSc', 2)}, 3: {('Finance', 21), ('Dance', 12)}, 7: {('Engineering', 7)}, 1: {('Geology', 10)}, 9: {('IT', 18)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Adele', 8, 'Arts'), ('Ed', 11, 'Health'), ('Joanna', 2, 'CompSc')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "\n",
    "HB_join(R, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Parallel Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cpQYKvvH241"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def consolidate_result(results, result):\n",
    "    # This is called whenever HB_Join(T1, T2) returns a result.\n",
    "    # The results list is modified only by the main process, not the pool workers.\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "def DPBP_join(T1, T2, n_processor):\n",
    "    \"\"\"\n",
    "    Perform a disjoint partitioning-based parallel join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T2\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- the number of parallel processors\n",
    "\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    results = []\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # partition t1 and t2 using range partitioning\n",
    "\n",
    "    T1_subsets = range_partition(T1, [10, 20])\n",
    "\n",
    "    T2_subsets = range_partition(T2, [10, 20])\n",
    "\n",
    "    # apply local join for each processor\n",
    "\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "\n",
    "    # iterate over the subsets\n",
    "\n",
    "    for i in range(len(T1_subsets)):\n",
    "\n",
    "        # perform hb_join on partitioned subsets\n",
    "\n",
    "        result = pool.apply_async(HB_join, [T1_subsets[i], T2_subsets[i]])\n",
    "\n",
    "        # append the result to the results list\n",
    "\n",
    "        output = result.get()\n",
    "\n",
    "        # append the result\n",
    "\n",
    "        results.append(output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJgTe8pVH_0z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {('CompSc', 2)}, 7: {('Engineering', 7)}, 8: {('Arts', 8)}}\n",
      "{1: {('Geology', 10)}, 2: {('Health', 11)}, 3: {('Dance', 12)}, 6: {('Business', 15)}, 9: {('IT', 18)}}\n",
      "{3: {('Finance', 21)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Joanna', 2, 'CompSc'), ('Adele', 8, 'Arts')], [('Ed', 11, 'Health')], []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processor = 3\n",
    "DPBP_join(R, S, n_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Duplication Outer Join Algorithm (DOJA)\n",
    "\n",
    "In this task, you will implement a **Duplication Outer Join Algorithm (DOJA)**. This algorithm consist of four main steps.\n",
    "\n",
    "1. (Replication): Duplicate the small table\n",
    "2. (Local Inner Join): Perform inner join in each processor\n",
    "3. (Distribution): Reshuffle the inner join result based on R.x\n",
    "4. (Local Outer Join): Perform outer join between the initial table R and the inner join result\n",
    "\n",
    "Assume **n_processor=3** and the dataset is initially partition using the **hash** method. For joining technique, please use the provided **outer join** function for the inner join and outer join operation. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R consists of 15 pairs, each comprising two attributes (nominal and numeric)\n",
    "\n",
    "R = [\n",
    "    (\"Adele\", 8),\n",
    "    (\"Bob\", 22),\n",
    "    (\"Clement\", 16),\n",
    "    (\"Dave\", 23),\n",
    "    (\"Ed\", 11),\n",
    "    (\"Fung\", 25),\n",
    "    (\"Goel\", 3),\n",
    "    (\"Harry\", 17),\n",
    "    (\"Irene\", 14),\n",
    "    (\"Joanna\", 2),\n",
    "    (\"Kelly\", 6),\n",
    "    (\"Lim\", 20),\n",
    "    (\"Meng\", 1),\n",
    "    (\"Noor\", 5),\n",
    "    (\"Omar\", 19),\n",
    "]\n",
    "\n",
    "# S consists of 8 pairs, each comprising two attributes (nominal and numeric)\n",
    "\n",
    "S = [\n",
    "    (\"Arts\", 8),\n",
    "    (\"Business\", 15),\n",
    "    (\"CompSc\", 2),\n",
    "    (\"Dance\", 12),\n",
    "    (\"Engineering\", 7),\n",
    "    (\"Finance\", 21),\n",
    "    (\"Geology\", 10),\n",
    "    (\"Health\", 11),\n",
    "    (\"IT\", 18),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def H(r):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process works\n",
    "    by summing the first and second digits of the hashed attribute, which\n",
    "    in this case is the join attribute.\n",
    "\n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its join attribute\n",
    "\n",
    "    Return:\n",
    "    result -- the hash index of the record r\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the value of the join attribute into the digits\n",
    "\n",
    "    digits = [int(d) for d in str(r[1])]\n",
    "\n",
    "    # Calulate the sum of elemenets in the digits\n",
    "\n",
    "    return sum(digits)\n",
    "\n",
    "\n",
    "def outer_join(L, R, join=\"left\"):\n",
    "    \"\"\"outer join using Hash-based join algorithm\"\"\"\n",
    "\n",
    "    if join == \"right\":\n",
    "        L, R = R, L\n",
    "    # inner join\n",
    "\n",
    "    if join == \"inner\":\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "        result = []\n",
    "        for l in L:\n",
    "            h_l = H(l)\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[0]])\n",
    "        return result\n",
    "    elif join in [\"left\", \"right\"]:\n",
    "        h_dic = {}\n",
    "        for r in R:\n",
    "            print(r)\n",
    "            h_r = H(r)\n",
    "            if h_r in h_dic.keys():\n",
    "                h_dic[h_r].add(r)\n",
    "            else:\n",
    "                h_dic[h_r] = {r}\n",
    "        print(h_dic)\n",
    "\n",
    "        result = []\n",
    "        for l in L:\n",
    "            isFound = False  # to check whether there is a match found.\n",
    "            h_l = H(l)\n",
    "\n",
    "            if h_l in h_dic.keys():\n",
    "                for item in h_dic[h_l]:\n",
    "                    if item[1] == l[1]:\n",
    "                        result.append([l[0], item[1], item[2]])\n",
    "                        isFound = True\n",
    "                        break\n",
    "            if not isFound:\n",
    "                result.append([l[0], l[1], str(np.nan)])\n",
    "        return result\n",
    "    else:\n",
    "        raise AttributeError(\"join should be in {left, right, inner}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_distribution(T, n):\n",
    "    \"\"\"distribute data using hash partitioning\"\"\"\n",
    "\n",
    "    # Define a simple hash function for demonstration\n",
    "\n",
    "    def s_hash(x, n):\n",
    "        h = x % n\n",
    "        return h\n",
    "\n",
    "    result = {}\n",
    "    for t in T:\n",
    "        h_key = s_hash(t[1], n)\n",
    "        if h_key in result.keys():\n",
    "            result[h_key].add(tuple(t))\n",
    "        else:\n",
    "            result[h_key] = {tuple(t)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def DOJA(L, R, n):\n",
    "    \"\"\"left outer join using DOJA\"\"\"\n",
    "\n",
    "    ### Start CODE\n",
    "\n",
    "    # step 1: determine the smaller relation and broadcast it to all processors\n",
    "\n",
    "    if len(L) < len(R):\n",
    "\n",
    "        # broadcast L to all processors\n",
    "\n",
    "        broadcasted_table = [tuple(s) for s in L]\n",
    "\n",
    "        # R becomes the larger table\n",
    "\n",
    "        L = [tuple(r) for r in R]\n",
    "    else:\n",
    "\n",
    "        # broadcast R to all processors\n",
    "\n",
    "        broadcasted_table = [tuple(s) for s in R]\n",
    "\n",
    "        # L remains the larger table\n",
    "\n",
    "        L = [tuple(l) for l in L]\n",
    "    # create a dictionary to store the broadcasted relation\n",
    "\n",
    "    broadcasted_dict = {i: broadcasted_table for i in range(n)}\n",
    "\n",
    "    # step 2: perform local inner join on each processor\n",
    "\n",
    "    local_inner_join_results = {}\n",
    "\n",
    "    # hash distribution of the larger table L\n",
    "\n",
    "    partitioned_L = hash_distribution(L, n)\n",
    "\n",
    "    for processor_id in range(n):\n",
    "\n",
    "        # inner join on each processor\n",
    "\n",
    "        local_inner_join_results[processor_id] = outer_join(\n",
    "            partitioned_L[processor_id], broadcasted_dict[processor_id], join=\"inner\"\n",
    "        )\n",
    "    # step 3: redistribute inner join results based on the join attribute from the larger relation\n",
    "\n",
    "    redistributed_inner_join_results = {}\n",
    "\n",
    "    for processor_id in range(n):\n",
    "\n",
    "        for inner_join_tuple in local_inner_join_results[processor_id]:\n",
    "\n",
    "            # apply hash function on the join attribute of the tuple\n",
    "\n",
    "            hash_value = H((None, inner_join_tuple[1]))\n",
    "\n",
    "            if hash_value in redistributed_inner_join_results:\n",
    "\n",
    "                redistributed_inner_join_results[hash_value].append(inner_join_tuple)\n",
    "            else:\n",
    "\n",
    "                redistributed_inner_join_results[hash_value] = [inner_join_tuple]\n",
    "    # step 4: perform local left outer join\n",
    "\n",
    "    final_output = {}\n",
    "\n",
    "    for processor_id in range(n):\n",
    "\n",
    "        final_output[processor_id] = []\n",
    "\n",
    "        # create a set of join attributes from broadcasted_table that found a match\n",
    "\n",
    "        matched_join_attributes = {\n",
    "            join_tuple[1] for join_tuple in local_inner_join_results[processor_id]\n",
    "        }\n",
    "\n",
    "        # include tuples from L that either match with broadcasted_table or are appended with np.nan\n",
    "\n",
    "        for L_tuple in partitioned_L[processor_id]:\n",
    "\n",
    "            if L_tuple[1] in matched_join_attributes:\n",
    "\n",
    "                # if a match was found, keep the matched tuple from local_inner_join_results\n",
    "\n",
    "                matched_tuple = next(\n",
    "                    (\n",
    "                        join_tuple\n",
    "                        for join_tuple in local_inner_join_results[processor_id]\n",
    "                        if join_tuple[1] == L_tuple[1]\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                final_output[processor_id].append(list(matched_tuple))\n",
    "            else:\n",
    "\n",
    "                # if no match was found, append np.nan\n",
    "\n",
    "                final_output[processor_id].append(list(L_tuple) + [np.nan])\n",
    "    ### End CODE\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [['Kelly', 6, nan], ['Goel', 3, nan]],\n",
       " 1: [['Fung', 25, nan],\n",
       "  ['Omar', 19, nan],\n",
       "  ['Meng', 1, nan],\n",
       "  ['Clement', 16, nan],\n",
       "  ['Bob', 22, nan]],\n",
       " 2: [['Joanna', 2, 'CompSc'],\n",
       "  ['Lim', 20, nan],\n",
       "  ['Adele', 8, 'Arts'],\n",
       "  ['Ed', 11, 'Health'],\n",
       "  ['Noor', 5, nan],\n",
       "  ['Harry', 17, nan],\n",
       "  ['Irene', 14, nan],\n",
       "  ['Dave', 23, nan]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOJA(R, S, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Parallel Join Variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following question.\n",
    "\n",
    "1. For each partitioning algorithm besides range-partitioning, state and justify whether we can use it with the code for disjoint-partitioning based parallel join above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:\n",
    "\n",
    "1. Round-robin data partitioning: Round-robin partitioning can be used with disjoint-partitioning based parallel join. It would require replacing the `range_partition` function with `rr_partition` function that allocates records to processing elements equally in clockwise manner. The join condition in the `HB_join` function would need to be modified to handle the non-partitioned join attributes. Round-robin partitioning is suitable when the join attributes are not known in advance or not used for partitioning.\n",
    "\n",
    "2. Hash data partitioning: Hash partitioning can be used with the given code by replacing the `range_partition` function with the `h_partition` function that applies a hash function to a specific attribute. Also, the `HB_join` function would need to be modified to use the same hash function for joining the partitions. Hash partitioning is suitable for equi-joins on the hashed attribute, as it groups records with the same hash value into the same partition.\n",
    "\n",
    "3. Random-unequal data partitioning: Random-unequal partitioning is not suitable as it assumes disjoint partitions based on a specific attribute. Random-unequal partitioning randomly allocates records to partitions, resulting in uneven distribution and potential data skew, which can affect the performance of the parallel join algorithm.\n",
    "\n",
    "4. Hybrid-Range Partitioning Strategy (HRPS): HRPS can be used with the given code by replacing the `range_partition` function with an `hrps_partition` function. HRPS combines range partitioning with hash and round-robin partitioning, partitioning records into small fragments based on a range of attribute values and distributing the fragments among processors in a round-robin fashion.\n",
    "\n",
    "5. MAGIC (Multiattribute Grid Declustering): MAGIC is not directly compatible as it assumes single-attribute partitioning. MAGIC partitions records based on multiple attributes using a grid structure, supporting both range and exact match queries on the partitioning attributes. The join operation would need to consider the grid structure and the multiple attributes for partitioning and joining.\n",
    "\n",
    "6. BERD (Bubba's Extended Range Declustering): BERD is not suitable as it alsoa assumes single-attribute partitioning. BERD uses two levels of partitioning: primary (range) and secondary (range on another attribute), and constructs an auxiliary table for the secondary attribute. Modifying the partitioning logic to handle the two-level partitioning and the auxiliary table would be necessary. The join operation would also need to consider the secondary attribute for partitioning and joining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9vIqdbjJaXv"
   },
   "source": [
    "## 3. Parallel Sorting Algorithm (2 marks)\n",
    "\n",
    "In this task, you will implement **parallel binary-merge sort** method. It has two phases same as the parallel merge-all sort that you learnt in the labs.\n",
    "\n",
    "1. Local sort\n",
    "2. Final merge.\n",
    "\n",
    "The first phase is similar to the parallel merge-all sort. The second phase, the merging phase, is pipelined instead of concentrating on one processor. In this phase, we take the results from two processors and then merge the two in one processor, called binary merging. The result of the merge between two processors is passed on to the next level until one processor (the host) is left.\n",
    "\n",
    "**Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n",
    "Assume that we use the round robin partitioning method (i.e. `rr_partition()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Round-robin Partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5p8cRn73Zqc"
   },
   "outputs": [],
   "source": [
    "# round-robin data partitioning function\n",
    "\n",
    "\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # iterate over each data point in the input dataset\n",
    "\n",
    "    for index, element in enumerate(data):\n",
    "\n",
    "        # calculate the index of the partition to place the current data point\n",
    "\n",
    "        index_bin = (int)(index % n)\n",
    "\n",
    "        # append the current data point to the corresponding partition\n",
    "\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk-Ch1Jm3iCC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)],\n",
      " [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)],\n",
      " [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n"
     ]
    }
   ],
   "source": [
    "# Test the round-robin partitioning function\n",
    "\n",
    "result = rr_partition(R, 3)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Serial Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd16AZF_LgWp"
   },
   "outputs": [],
   "source": [
    "def qsort(arr):\n",
    "    \"\"\"\n",
    "    Quicksort a list\n",
    "\n",
    "    Arguments:\n",
    "    arr -- the input list to be sorted\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted arr\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    else:\n",
    "        # take the first element as the pivot\n",
    "\n",
    "        pivot = arr[0]\n",
    "        left_arr = [x for x in arr[1:] if x[1] < pivot[1]]\n",
    "        right_arr = [x for x in arr[1:] if x[1] >= pivot[1]]\n",
    "        # uncomment this to see what to print\n",
    "        # print(\"Left:\" + str(left_arr)+\" Pivot : \"+ str(pivot)+\" Right: \" + str(right_arr))\n",
    "\n",
    "        sorted_arr = qsort(left_arr) + [pivot] + qsort(right_arr)\n",
    "\n",
    "        return sorted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8kIDuxV1uGS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Meng', 1),\n",
       " ('Joanna', 2),\n",
       " ('Goel', 3),\n",
       " ('Noor', 5),\n",
       " ('Kelly', 6),\n",
       " ('Adele', 8),\n",
       " ('Ed', 11),\n",
       " ('Irene', 14),\n",
       " ('Clement', 16),\n",
       " ('Harry', 17),\n",
       " ('Omar', 19),\n",
       " ('Lim', 20),\n",
       " ('Bob', 22),\n",
       " ('Dave', 23),\n",
       " ('Fung', 25)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qsort(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3vxcrs-LVaG"
   },
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used\n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# Find the smallest record\n",
    "\n",
    "\n",
    "def find_min(records):\n",
    "    \"\"\"\n",
    "    Find the smallest record\n",
    "\n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    m = records[0]\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if records[i][1] < m[1]:\n",
    "            index = i\n",
    "            m = records[i]\n",
    "    return index\n",
    "\n",
    "\n",
    "def k_way_merge(record_sets):\n",
    "    \"\"\"\n",
    "    K-way merging algorithm\n",
    "\n",
    "    Arguments:\n",
    "    record_sets -- the set of multiple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "\n",
    "    indexes = []\n",
    "    for _ in record_sets:\n",
    "        indexes.append(0)  # initialisation with 0\n",
    "    # final result will be stored in this variable\n",
    "\n",
    "    result = []\n",
    "    while True:\n",
    "        merged_result = []  # the merging unit (i.e. # of the given buffers)\n",
    "\n",
    "        # This loop gets the current position of every buffer\n",
    "\n",
    "        for i in range(len(record_sets)):\n",
    "            if indexes[i] >= len(record_sets[i]):\n",
    "                merged_result.append((None, sys.maxsize))\n",
    "            else:\n",
    "                merged_result.append(record_sets[i][indexes[i]])\n",
    "        # find the smallest record\n",
    "\n",
    "        smallest = find_min(merged_result)\n",
    "\n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "\n",
    "        if merged_result[smallest][1] == sys.maxsize:\n",
    "            break\n",
    "        # This record is the next on the merged list\n",
    "\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMrHqU8o3Mh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adele', 8), ('Dave', 23), ('Goel', 3), ('Joanna', 2), ('Meng', 1)], [('Bob', 22), ('Ed', 11), ('Harry', 17), ('Kelly', 6), ('Noor', 5)], [('Clement', 16), ('Fung', 25), ('Irene', 14), ('Lim', 20), ('Omar', 19)]]\n",
      "[('Meng', 1),\n",
      " ('Joanna', 2),\n",
      " ('Goel', 3),\n",
      " ('Noor', 5),\n",
      " ('Kelly', 6),\n",
      " ('Adele', 8),\n",
      " ('Ed', 11),\n",
      " ('Irene', 14),\n",
      " ('Clement', 16),\n",
      " ('Harry', 17),\n",
      " ('Omar', 19),\n",
      " ('Lim', 20),\n",
      " ('Bob', 22),\n",
      " ('Dave', 23),\n",
      " ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "# Test k-way merging method\n",
    "\n",
    "buffers = rr_partition(R, 3)\n",
    "print(buffers)\n",
    "result = k_way_merge([qsort(b) for b in buffers])\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yof8Q84YLcOU"
   },
   "outputs": [],
   "source": [
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    perform a serial external sorting method based on sort-merge\n",
    "    the buffer size determines the size of each sub-record set\n",
    "\n",
    "    arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "\n",
    "    if buffer_size <= 2:\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    sorted_set = []\n",
    "\n",
    "    # sort phase\n",
    "\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "\n",
    "    while start_pos < N:\n",
    "\n",
    "        # read B-records from the input, where B = buffer_size\n",
    "        subset = dataset[start_pos : start_pos + buffer_size]\n",
    "\n",
    "        # sort the subset using quicksort\n",
    "        sorted_subset = qsort(subset)\n",
    "\n",
    "        # append the sorted subset to the sorted set\n",
    "        sorted_set.append(sorted_subset)\n",
    "\n",
    "        # update the starting position to the next position\n",
    "        start_pos += buffer_size\n",
    "\n",
    "    # merge phase\n",
    "\n",
    "    # merge buffer size is one less than the buffer size to ensure that the memory is enough to store the merged records\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "\n",
    "    # iterate until the dataset is completely merged\n",
    "    while len(dataset) > 1:\n",
    "\n",
    "        merged_set = []\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "\n",
    "        while start_pos < N:\n",
    "\n",
    "            # read c-record sets from merged record, where C = merge_buffer_size\n",
    "            subset = dataset[start_pos : start_pos + merge_buffer_size]\n",
    "\n",
    "            # merge the subset using k-way merge\n",
    "            merged_set.append(k_way_merge(subset))\n",
    "\n",
    "            # update the starting position to the next position\n",
    "            start_pos += merge_buffer_size\n",
    "\n",
    "        # update the dataset with the merged set\n",
    "        dataset = merged_set\n",
    "\n",
    "    # the final result is the first (and only) element in the dataset\n",
    "    result = dataset[0]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adele', 8), ('Bob', 22), ('Clement', 16), ('Dave', 23), ('Ed', 11), ('Fung', 25), ('Goel', 3), ('Harry', 17), ('Irene', 14), ('Joanna', 2), ('Kelly', 6), ('Lim', 20), ('Meng', 1), ('Noor', 5), ('Omar', 19)]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmW47ALF6bH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final sorting result:[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "result = serial_sorting(R, 4)\n",
    "print(\"final sorting result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Parallel Binary-merge Sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_jH8jXwLKRT"
   },
   "outputs": [],
   "source": [
    "# Include this package for parallel processing\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def parallel_binary_merge_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel binary-merge sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "\n",
    "    if buffer_size <= 2:\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # partition using round-robin partitioning\n",
    "\n",
    "    data_sets = rr_partition(dataset, n_processor)\n",
    "\n",
    "    # sort phase\n",
    "\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "\n",
    "    sorted_lists = []\n",
    "    parallel_result = []\n",
    "\n",
    "    # apply serial sorting on each processor\n",
    "\n",
    "    for i, data_set in enumerate(data_sets):\n",
    "        parallel_result.append(\n",
    "            pool.apply_async(serial_sorting, [data_set, buffer_size])\n",
    "        )\n",
    "    # get the sorted sub-record sets\n",
    "\n",
    "    for i, process in enumerate(parallel_result):\n",
    "        sorted_list = process.get()\n",
    "        sorted_lists.append(sorted_list)\n",
    "    pool.close()\n",
    "\n",
    "    # merge phase\n",
    "\n",
    "    iteration = 1\n",
    "\n",
    "    # merge the sorted sub-record sets using k-way merge\n",
    "\n",
    "    while len(sorted_lists) > 1:\n",
    "\n",
    "        # calculate the number of merges\n",
    "\n",
    "        num_merges = len(sorted_lists) // 2\n",
    "\n",
    "        # check if the number of sorted sub-record sets is odd\n",
    "\n",
    "        odd_list = None\n",
    "\n",
    "        # if the number of sorted sub-record sets is odd, pop the last one to make it even\n",
    "\n",
    "        if len(sorted_lists) % 2 != 0:\n",
    "            odd_list = sorted_lists.pop()\n",
    "        pool = mp.Pool(processes=num_merges)\n",
    "        merge_tasks = []\n",
    "\n",
    "        # apply k-way merge on each processor\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            merge_tasks.append(\n",
    "                pool.apply_async(\n",
    "                    k_way_merge, ([sorted_lists[i * 2], sorted_lists[i * 2 + 1]],)\n",
    "                )\n",
    "            )\n",
    "        # get the merged sub-record sets\n",
    "\n",
    "        merged_lists = [task.get() for task in merge_tasks]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # append the odd list if it exists\n",
    "\n",
    "        if odd_list:\n",
    "            merged_lists.append(odd_list)\n",
    "        # update the sorted sub-record sets\n",
    "\n",
    "        sorted_lists = merged_lists\n",
    "        iteration += 1\n",
    "    result = sorted_lists[0]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMu19MwXLxNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:[('Meng', 1), ('Joanna', 2), ('Goel', 3), ('Noor', 5), ('Kelly', 6), ('Adele', 8), ('Ed', 11), ('Irene', 14), ('Clement', 16), ('Harry', 17), ('Omar', 19), ('Lim', 20), ('Bob', 22), ('Dave', 23), ('Fung', 25)]\n"
     ]
    }
   ],
   "source": [
    "result = parallel_binary_merge_sorting(R, 10, 20)\n",
    "print(\"final result:\" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Parallel Binary-merge Behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer the following.\n",
    "\n",
    "1. Does parallel binary-merge utilize all available processors? State and justify with respect to each phase: sort, merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:\n",
    "\n",
    "Parallel binary-merge sort does not fully utilize all available processors throughout the entire sorting process.\n",
    "\n",
    "**Sort:**\n",
    "\n",
    "In the sort phase, the input data is first partitioned across all processors using a round-robin scheme, ensuring an even distribution of data among the processors. Each processor then independently performs a serial external sort on its local data partition. Local sorting in each processor is carried out using a normal serial external sorting mechanism, as the data is assumed to be too large to fit in main memory. During the local sort phase, all processors are actively sorting their respective data partitions simultaneously. There is no idle time or underutilization of processors. Therefore, the local sort phase achieves excellent utilization of all available processors, as they are all working in parallel to sort their assigned data partitions.\n",
    "\n",
    "**Merge:**\n",
    "\n",
    "The merging phase in parallel binary-merge sort follows a pipelined, hierarchical approach. The merging process continues iteratively until only one sorted list remains. In each iteration, the number of sorted sublists is halved by merging pairs of sublists. If the number of sublists is odd, the last sublist is kept aside and merged in the next iteration. The merging tasks are distributed among a number of processors equal to half the number of sublists. Each processor merges a pair of sublists using the k_way_merge function. However, as the merging progresses up the binary merge tree, the number of active processors decreases by half at each level. In the final merge step, only a single processor remains to merge the last two sorted sublists. Throughout the merge phase, not all processors are actively utilized. At each level of the merge tree, half of the processors from the previous level become idle. Moreover, there is still no true parallelism in the merging because only a subset, not all, of the available processors are used.\n",
    "\n",
    "In summary, parallel binary-merge sort achieves excellent processor utilization during the local sort phase, where all processors are actively sorting their respective data partitions in parallel. However, during the merge phase, processor utilization decreases as the merging progresses up the binary merge tree. At each level of the merge tree, half of the processors become idle. The final merge step is performed by a single processor. So while the pipelined merging in parallel binary-merge sort improves upon the single-processor merging in parallel merge-all sort by distributing the merging workload among multiple processors, it still fails to achieve optimal utilization of all available processors throughout the entire merging process.\n",
    "\n",
    "Parallel binary-merge sort does not fully utilize all available processors, particularly during the merge phase, due to the hierarchical binary merge approach that results in a decreasing number of active processors at each level of the merge tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Parallel GroupBy (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parallel GroupBy with Merge-All\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Merge-All** method. It invloves the following two steps:\n",
    "\n",
    "1. Local aggregate in each processor\n",
    "2. Global aggregation\n",
    "\n",
    "Assume **n_processor=4** and the dataset has already been pre-partitioned. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = [(\"A\", 1), (\"B\", 2), (\"C\", 3), (\"A\", 10), (\"B\", 20), (\"C\", 30)]\n",
    "D2 = [(\"A\", 2), (\"B\", 3), (\"C\", 4), (\"A\", 20), (\"B\", 30), (\"C\", 40)]\n",
    "D3 = [(\"A\", 3), (\"B\", 4), (\"C\", 5), (\"A\", 30), (\"B\", 40), (\"C\", 50)]\n",
    "D4 = [(\"A\", 4), (\"B\", 5), (\"C\", 6), (\"A\", 40), (\"B\", 50), (\"C\", 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step in the merge-all groupby method\n",
    "\n",
    "\n",
    "def local_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a local groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[0]\n",
    "        val = record[1]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        dict[key] += val\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 11, 'B': 22, 'C': 33}\n"
     ]
    }
   ],
   "source": [
    "result = local_groupby(D1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # number of processors is equal to the number of records in the dataset\n",
    "\n",
    "    n_processor = len(dataset)\n",
    "    pool = mp.Pool(processes=n_processor)\n",
    "\n",
    "    #  local aggregation\n",
    "\n",
    "    local_result = []\n",
    "\n",
    "    # apply local_groupby on each processor\n",
    "\n",
    "    for s in dataset:\n",
    "\n",
    "        # apply local_groupby on each processor\n",
    "\n",
    "        local_result.append(pool.apply(local_groupby, [s]))\n",
    "    pool.close()\n",
    "\n",
    "    # global aggregation\n",
    "\n",
    "    # iterate over the local results\n",
    "\n",
    "    for r in local_result:\n",
    "\n",
    "        for key, val in r.items():\n",
    "\n",
    "            # if the key exists\n",
    "\n",
    "            if key not in result:\n",
    "\n",
    "                # add the key to the result\n",
    "\n",
    "                result[key] = 0\n",
    "            # add the value to the existing key\n",
    "\n",
    "            result[key] += val\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 110, 'B': 154, 'C': 198}\n"
     ]
    }
   ],
   "source": [
    "E = [D1, D2, D3, D4]\n",
    "result = parallel_merge_all_groupby(E)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Redistribution Method\n",
    "\n",
    "In this task, you will implement **Parallel GroupBy with Redistribution Method** method. It invloves the following two steps:\n",
    "\n",
    "1. (Partitioning phase): Redistribute raw records to all processor\n",
    "2. (Aggregation phase): Each processor performs a local aggregation\n",
    "\n",
    "Assume **n_processor=4** and the dataset should be partitioned into 4 groups using **range partition** Where first group with A & B, second with C & D, third with E & F and fourth with G & H. For local aggregation, please use the provided **local_groupby** function. **Complete the code block between `### START CODE HERE ###` and `### END CODE HERE ###`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [\n",
    "    (\"A\", 1),\n",
    "    (\"B\", 2),\n",
    "    (\"C\", 3),\n",
    "    (\"D\", 10),\n",
    "    (\"E\", 20),\n",
    "    (\"F\", 30),\n",
    "    (\"G\", 20),\n",
    "    (\"H\", 30),\n",
    "    (\"A\", 2),\n",
    "    (\"B\", 3),\n",
    "    (\"C\", 4),\n",
    "    (\"D\", 20),\n",
    "    (\"E\", 30),\n",
    "    (\"F\", 40),\n",
    "    (\"G\", 30),\n",
    "    (\"H\", 40),\n",
    "    (\"A\", 3),\n",
    "    (\"B\", 4),\n",
    "    (\"C\", 5),\n",
    "    (\"D\", 30),\n",
    "    (\"E\", 40),\n",
    "    (\"F\", 50),\n",
    "    (\"G\", 40),\n",
    "    (\"H\", 50),\n",
    "    (\"A\", 4),\n",
    "    (\"B\", 5),\n",
    "    (\"C\", 6),\n",
    "    (\"D\", 40),\n",
    "    (\"E\", 50),\n",
    "    (\"F\", 60),\n",
    "    (\"G\", 50),\n",
    "    (\"H\", 60),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create range partition function based on GroupBy attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range data partitionining function (Need to modify as instructed above)\n",
    "\n",
    "\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data based on the join attribute\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset (list of tuple)\n",
    "    range_indices -- the range end-points (e.g., ['C', 'E', 'G'] means\n",
    "                        4 intervals: x < 'C', 'C' <= x < 'E', 'E' <= x < 'G', 'G' <= x)\n",
    "\n",
    "    Return:\n",
    "    result -- the partitioned subsets of D\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # sort the dataset according to their values\n",
    "\n",
    "    new_data = list(data)\n",
    "    new_data.sort()\n",
    "\n",
    "    # calculate the number of bins\n",
    "\n",
    "    n_bins = len(range_indices)\n",
    "\n",
    "    # for each bin\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        # find elements belonging to each range\n",
    "\n",
    "        s = [x for x in new_data if x[0] < range_indices[i]]\n",
    "\n",
    "        # add the partitioned list to the result\n",
    "\n",
    "        result.append(s)\n",
    "\n",
    "        # find the last element in the previous partition\n",
    "\n",
    "        last_element = s[-1] if s else None\n",
    "\n",
    "        # find the index of the last element\n",
    "\n",
    "        if last_element:\n",
    "            last_index = new_data.index(last_element)\n",
    "\n",
    "            # remove the partitioned list from the dataset\n",
    "\n",
    "            new_data = new_data[last_index + 1 :]\n",
    "        else:\n",
    "            new_data = new_data\n",
    "    # append the last remaining data list\n",
    "\n",
    "    result.append([x for x in new_data if x[0] >= range_indices[n_bins - 1]])\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def parallel_redistributed_groupby(dataset, range_indices):\n",
    "    \"\"\"\n",
    "    Perform a parallel redistributed groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # partition the dataset into 4 groups using range partitioning\n",
    "\n",
    "    partitioned_data = range_partition(dataset, range_indices)\n",
    "\n",
    "    # create with 4 processes\n",
    "\n",
    "    pool = mp.Pool(processes=4)\n",
    "\n",
    "    # perform local aggregation on each partition in parallel\n",
    "\n",
    "    local_results = pool.map(local_groupby, partitioned_data)\n",
    "\n",
    "    # close the pool\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # union the local results into the final result\n",
    "\n",
    "    result = {k: v for d in local_results for k, v in d.items()}\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 10, 'B': 14, 'C': 18, 'D': 100, 'E': 140, 'F': 180, 'G': 140, 'H': 180}\n"
     ]
    }
   ],
   "source": [
    "result = parallel_redistributed_groupby(D, [\"C\", \"E\", \"G\"])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT3182 - Take_Home_Test_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
